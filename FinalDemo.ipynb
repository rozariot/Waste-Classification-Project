{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalDemo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "408402bf34b748fb9b4c42f40548471d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9839982a089946d2a7a3d4718ee5302b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_47c773fe4e8840a3a95eedfe69f68dac",
              "IPY_MODEL_7ee08237bf36471b834a7faacad0d98b"
            ]
          }
        },
        "9839982a089946d2a7a3d4718ee5302b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "47c773fe4e8840a3a95eedfe69f68dac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ea091c881e3c4fe79ded878b7aa85eed",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 241530880,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 241530880,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3cab2ed02c814da1ab2d21082ca946df"
          }
        },
        "7ee08237bf36471b834a7faacad0d98b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e593cde8b1ad4cfdb54601178461a9c9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 230M/230M [00:09&lt;00:00, 24.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a7081dc7f3734b5ebf3c9e4d676c3cb0"
          }
        },
        "ea091c881e3c4fe79ded878b7aa85eed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3cab2ed02c814da1ab2d21082ca946df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e593cde8b1ad4cfdb54601178461a9c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a7081dc7f3734b5ebf3c9e4d676c3cb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOp6NT5fmAYf",
        "outputId": "04855b40-b1d2-4d06-d34c-d130a959c8a5"
      },
      "source": [
        "# import important libraries\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from math import floor\n",
        "import os\n",
        "import torchvision.models\n",
        "import copy\n",
        "use_cuda = True #enable usage of cuda GPU\n",
        "torch.manual_seed(1000)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f21e05bed90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2srvaHels3cl"
      },
      "source": [
        "# need to upload your initial video, your Best_Model and manually add files frames (not used) and newframes (used)\n",
        "VIDEO_LOADER_PATH=\"/content/secondtrashvid.mp4\"\n",
        "VIDEO_MODDED_PATH=\"/content/secondtrashdemowithlabels.mp4\"\n",
        "MODEL_PATH=\"/content/Best_Model\"\n",
        "frames_path=\"/content/frames/\"\n",
        "new_frames_path=\"/content/newframes/\""
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52BHzP7EmOxH"
      },
      "source": [
        "classes = ['compost_food',\n",
        " 'compost_other',\n",
        " 'garbage',\n",
        " 'recy_bags',\n",
        " 'recy_bottles',\n",
        " 'recy_cans',\n",
        " 'recy_cups',\n",
        " 'recy_other_containers',\n",
        " 'recy_paper',\n",
        " 'recy_styrofoam']\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54pPCr6uufxe"
      },
      "source": [
        "#get frames of video\n",
        "import cv2\n",
        "vidcap = cv2.VideoCapture(VIDEO_LOADER_PATH)\n",
        "frames=[]\n",
        "def getFrame(sec):\n",
        "    vidcap.set(cv2.CAP_PROP_POS_MSEC,sec*1000)\n",
        "    hasFrames,image = vidcap.read()\n",
        "    if hasFrames:\n",
        "        image=cv2.resize(image,(224,224))\n",
        "        frames.append(image)    # save frame as JPG file\n",
        "        \n",
        "        cv2.imwrite(frames_path+\"image\"+str(count)+\".jpg\", image)\n",
        "    return hasFrames\n",
        "sec = 0\n",
        "frameRate = .04 #//it will capture image in each 0.5 second\n",
        "count=1\n",
        "success = getFrame(sec)\n",
        "while success:\n",
        "    count = count + 1\n",
        "    sec = sec + frameRate\n",
        "    sec = round(sec, 2)\n",
        "    success = getFrame(sec)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zi0ITPKl4hjO"
      },
      "source": [
        "\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "408402bf34b748fb9b4c42f40548471d",
            "9839982a089946d2a7a3d4718ee5302b",
            "47c773fe4e8840a3a95eedfe69f68dac",
            "7ee08237bf36471b834a7faacad0d98b",
            "ea091c881e3c4fe79ded878b7aa85eed",
            "3cab2ed02c814da1ab2d21082ca946df",
            "e593cde8b1ad4cfdb54601178461a9c9",
            "a7081dc7f3734b5ebf3c9e4d676c3cb0"
          ]
        },
        "id": "kcLSwCoImouP",
        "outputId": "1210974c-8e18-4a25-a774-b58393a15e39"
      },
      "source": [
        "resnet152 = torchvision.models.resnet152(pretrained=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-b121ed2d.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "408402bf34b748fb9b4c42f40548471d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=241530880.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MbS7rBSmyg0"
      },
      "source": [
        "class Waste_Classifier_One_Linear_Layer(nn.Module):\n",
        "  def __init__(self, name=\"WasteNet\", hidden_size=256*6*6):\n",
        "    super(Waste_Classifier_One_Linear_Layer, self).__init__()\n",
        "    self.name = name\n",
        "    self.fc1 = nn.Linear(hidden_size, 10)\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.view(-1, self.hidden_size) #flatten feature data\n",
        "    x = self.fc1(x)\n",
        "    return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toF-PY92VuBS"
      },
      "source": [
        "resnet152 = torchvision.models.resnet152(pretrained=True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfwgWE41rjoe"
      },
      "source": [
        "\n",
        "resnet152 = nn.Sequential(*(list(resnet152.children())[0:9]))\n",
        "resnet152.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMcZWVeTRvwW"
      },
      "source": [
        "model_fc=Waste_Classifier_One_Linear_Layer(name=\"res152_best\", hidden_size=2048*1*1)\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model_fc = model_fc.cuda()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPNzq6ouTO7k",
        "outputId": "36c4a075-e121-4172-a7f4-32728105d679"
      },
      "source": [
        "model_fc.load_state_dict(torch.load(MODEL_PATH))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZFvK3-IZ4fL"
      },
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms.functional as TF"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDNEbafsm32l"
      },
      "source": [
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "transformations = transforms.Compose([                                     \n",
        "  transforms.Resize((224,224)),\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize(mean, std)  \n",
        "])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwRYXcaVnGEu"
      },
      "source": [
        "#get predicted values and add text to frame\n",
        "from PIL import Image\n",
        "from matplotlib import cm\n",
        "def get_predication(frame):\n",
        "  # print(type(frame))\n",
        "  image1 = Image.fromarray(frame)\n",
        "  \n",
        "\n",
        "  # print(image1)\n",
        "  # cv2_imshow(image1)\n",
        "  # image1=torch.from_numpy(image1)\n",
        "  input_image1 = transformations(image1)\n",
        "  if use_cuda and torch.cuda.is_available():\n",
        "    input_image1 = input_image1.cuda()\n",
        "    model_cuda = resnet152.cuda()\n",
        "    pred1 = model_cuda(input_image1[None, ...])\n",
        "  else:\n",
        "    pred1 = resnet152(input_image1[None, ...])\n",
        "  pred1_out = model_fc(pred1)\n",
        "\n",
        "  # print()\n",
        "  pred=pred1_out.softmax(1).tolist()\n",
        "  pred=[round(num,2) for num in pred[0]]\n",
        "  # print(list(zip(classes,pred)))\n",
        "  pred,cls= pred1_out.softmax(1).max(1, keepdim=True)[0],classes[pred1_out.softmax(1).max(1, keepdim=True)[1]]\n",
        "  # print(image1,pred,cls)\n",
        "  frame=cv2.putText(frame, \"class \"+cls+\" predn: \"+str(round(pred.tolist()[0][0],2)), (50,50), font, .3, \n",
        "            (0, 0, 0),1,\n",
        "            cv2.LINE_4)\n",
        "  return frame"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xa_1MM7BsKgg"
      },
      "source": [
        "#run through frames and call cell above. \n",
        "count=0\n",
        "# print(type(frames[0]))\n",
        "frame_array=frames.copy()\n",
        "for frame in frame_array:\n",
        "  # image1 = cv2.imread(frames_path+frame)\n",
        "  # cv2_imshow(image1)\n",
        "  \n",
        "  # pred , cls = \n",
        "\n",
        "  newimg=get_predication(frame)\n",
        "  # plt.imshow(newimg)\n",
        "  # cv2_imshow(newimg)\n",
        "  cv2.imwrite(new_frames_path+\"image\"+str(count)+\".jpg\", newimg)\n",
        "  count+=1\n"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUkT8YZ2uUIW"
      },
      "source": [
        "#convert new frames to video which you can then download\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from os.path import isfile, join\n",
        "\n",
        "\n",
        "fps = 24\n",
        "frame_array=[]\n",
        "for frame in frames:\n",
        "    # filename=VI + files[i]\n",
        "    #reading each files\n",
        "    frame=cv2.resize(frame,(224,224))\n",
        "    size = (224,224)\n",
        "    \n",
        "    #inserting the frames into an image array\n",
        "    frame_array.append(frame)\n",
        "out = cv2.VideoWriter(VIDEO_MODDED_PATH,cv2.VideoWriter_fourcc(*'DIVX'), fps, size)\n",
        "for i in range(len(frame_array)):\n",
        "    # writing to a image array\n",
        "    out.write(frame_array[i])\n",
        "out.release()"
      ],
      "execution_count": 73,
      "outputs": []
    }
  ]
}